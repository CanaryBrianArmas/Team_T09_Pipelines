{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "# Model Evaluation Notebook\n",
    "**Team:** [Your Team Name]  \n",
    "**Authors:** [Team Members]  \n",
    "**Date:** [Date]\n",
    "\n",
    "## Objective\n",
    "This notebook demonstrates loading a trained pipeline, making predictions on test data, and evaluating model performance.\n",
    "\"\"\"\n",
    "# %%\n",
    "# =====================\n",
    "# IMPORTS\n",
    "# =====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 1. Model Loading\n",
    "Utility functions for loading persisted pipeline\n",
    "\"\"\"\n",
    "# %%\n",
    "def load_model(file_path: str):\n",
    "    \"\"\"\n",
    "    Load trained pipeline from disk\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to saved model\n",
    "    \n",
    "    Returns:\n",
    "    Pipeline: Loaded sklearn pipeline\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"No model found at {file_path}\")\n",
    "        \n",
    "    model = joblib.load(file_path)\n",
    "    print(f\"Successfully loaded model from {file_path}\")\n",
    "    return model\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 2. Data Loading\n",
    "Load and prepare test data for prediction\n",
    "\"\"\"\n",
    "# %%\n",
    "def load_test_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and prepare test dataset\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to test CSV file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (features, target) pandas DataFrames\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded test data with shape: {test_df.shape}\")\n",
    "    \n",
    "    # Ensure same preprocessing as training\n",
    "    X_test = test_df.drop('target', axis=1)\n",
    "    y_test = test_df['target']\n",
    "    \n",
    "    return X_test, y_test\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 3. Prediction & Evaluation\n",
    "Generate predictions and calculate performance metrics\n",
    "\"\"\"\n",
    "# %%\n",
    "def make_predictions(model, X_test: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate predictions using trained pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained sklearn pipeline\n",
    "    X_test (pd.DataFrame): Test features\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: Array of predictions\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_test)\n",
    "    print(f\"Generated {len(predictions)} predictions\")\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(y_true: pd.Series, y_pred: np.ndarray, proba_pred: np.ndarray = None):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics and generate visualizations\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (pd.Series): True target values\n",
    "    y_pred (np.ndarray): Model predictions\n",
    "    proba_pred (np.ndarray): Predicted probabilities (for classification)\n",
    "    \"\"\"\n",
    "    # Classification Metrics\n",
    "    if np.issubdtype(y_true.dtype, np.number) and len(np.unique(y_true)) > 2:\n",
    "        # Regression Metrics\n",
    "        print(\"Regression Metrics:\")\n",
    "        print(f\"MSE: {mean_squared_error(y_true, y_pred):.3f}\")\n",
    "        print(f\"RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.3f}\")\n",
    "        print(f\"RÂ²: {r2_score(y_true, y_pred):.3f}\")\n",
    "    else:\n",
    "        # Classification Metrics\n",
    "        print(\"Classification Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n",
    "        print(f\"Precision: {precision_score(y_true, y_pred):.3f}\")\n",
    "        print(f\"Recall: {recall_score(y_true, y_pred):.3f}\")\n",
    "        print(f\"F1 Score: {f1_score(y_true, y_pred):.3f}\")\n",
    "        \n",
    "        if proba_pred is not None:\n",
    "            print(f\"ROC AUC: {roc_auc_score(y_true, proba_pred):.3f}\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        \n",
    "        # Confusion Matrix Visualization\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(confusion_matrix(y_true, y_pred), \n",
    "                    annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 4. Results Saving\n",
    "Save predictions and metrics for future reference\n",
    "\"\"\"\n",
    "# %%\n",
    "def save_results(y_pred: np.ndarray, file_path: str):\n",
    "    \"\"\"\n",
    "    Save predictions to CSV file\n",
    "    \n",
    "    Parameters:\n",
    "    y_pred (np.ndarray): Array of predictions\n",
    "    file_path (str): Path to save predictions\n",
    "    \"\"\"\n",
    "    results_df = pd.DataFrame(y_pred, columns=['predictions'])\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    results_df.to_csv(file_path, index=False)\n",
    "    print(f\"Predictions saved to {file_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 5. Execution\n",
    "Main execution flow for end-to-end evaluation\n",
    "\"\"\"\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    # Load artifacts\n",
    "    model = load_model('../models/best_pipeline.pkl')\n",
    "    X_test, y_test = load_test_data('../data/test.csv')\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = make_predictions(model, X_test)\n",
    "    \n",
    "    # For classification: get probabilities if available\n",
    "    proba_predictions = None\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        proba_predictions = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate performance\n",
    "    evaluate_model(y_test, predictions, proba_predictions)\n",
    "    \n",
    "    # Save results\n",
    "    save_results(predictions, '../results/predictions.csv')\n",
    "    print(\"Evaluation process completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
